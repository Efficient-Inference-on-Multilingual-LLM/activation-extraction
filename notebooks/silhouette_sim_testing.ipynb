{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa10866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Change to parent directory\n",
    "os.chdir('..')\n",
    "# Add current directory to path\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a354db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.utils.const import LANGCODE2LANGNAME, LANGNAME2LANGCODE, MODEL2HIDDEN_SIZE, MODEL2NUM_LAYERS, EXP2_CONFIG, EXP3_CONFIG\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fc65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemma-3-4b-it' # 1152 size for 1b, 2560 for 4b\n",
    "model_to_num_layers = {\n",
    "    'gemma-3-1b-it': 26,\n",
    "\t'gemma-3-4b-it': 34,\n",
    "    'gemma-3-270m-it': 18,\n",
    "    'gemma-2-9b-it': 42\n",
    "}\n",
    "model_to_hidden_size = {\n",
    "\t'gemma-3-1b-it': 1152,\n",
    "\t'gemma-3-4b-it': 2560,\n",
    "\t'gemma-3-270m-it': 640,\n",
    "\t'gemma-2-9b-it': 3584\n",
    "}\n",
    "num_layers = model_to_num_layers[model_name]\n",
    "extraction_mode = 'raw'\n",
    "token_position = 'last_token'\n",
    "languages = glob.glob('outputs/topic_classification/gemma-3-4b-it/raw/*')\n",
    "languages = sorted([lang.split('/')[-1] for lang in languages])\n",
    "text_ids = glob.glob('outputs/topic_classification/gemma-3-4b-it/raw/eng_Latn/*')\n",
    "text_ids = [text_id.split('/')[-1].split('.')[0] for text_id in text_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc264ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_path = 'outputs_flores_plus/next_token/dev/Qwen3-8B/raw/amh_Ethi/2/average/layer_residual-postattn_6.pt'\n",
    "activation = torch.load(activation_path)\n",
    "print(f'Activation shape: {activation.shape}')  # Should be (num_texts,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6240c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract language codes from the comment and add eng_Latn\n",
    "# languages = [\n",
    "# \t'vie_Latn',  # Vietnamese\n",
    "# \t'ind_Latn',  # Indonesian\n",
    "# \t'tha_Thai',  # Thai\n",
    "# \t'zsm_Latn',  # Malay\n",
    "# \t'mya_Mymr',  # Burmese -> 0 problem\n",
    "# \t'tgl_Latn',  # Tagalog\n",
    "# \t'khm_Khmr',  # Khmer\n",
    "# \t'ceb_Latn',  # Cebuano\n",
    "# \t'lao_Laoo',  # Lao\n",
    "# \t'jav_Latn',  # Javanese\n",
    "# \t'war_Latn',  # Waray\n",
    "# \t'sun_Latn',  # Sundanese\n",
    "# \t'ilo_Latn',  # Ilocano\n",
    "# \t'tam_Taml',  # Tamil\n",
    "# \t'zho_Hans',  # Chinese\n",
    "# \t'eng_Latn'   # English\n",
    "# ]\n",
    "\n",
    "languages = []\n",
    "for family, langs in EXP3_CONFIG['languages'].items():\n",
    "\tlanguages.extend(langs)\n",
    "\n",
    "languages = [LANGNAME2LANGCODE[lang] for lang in languages]\n",
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Text IDs: {len(text_ids)}, Num layers: {num_layers}, Number of languages: {len(languages)}, Hidden size: {model_to_hidden_size[model_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty torch tensor to hold all activations [text_id, layer_id, language, hidden_size]\n",
    "activation_per_lang = torch.zeros((len(text_ids), num_layers + 1, len(languages), MODEL2HIDDEN_SIZE[model_name])).to('cuda')\n",
    "print(f'Activation tensor shape: {activation_per_lang.shape}')\n",
    "\n",
    "# Reshape to [text_id * language, layer_id, hidden_size]\n",
    "activation_per_lang = activation_per_lang.view(-1, num_layers + 1, MODEL2HIDDEN_SIZE[model_name])\n",
    "print(f'Reshaped activation tensor shape: {activation_per_lang.shape}')\n",
    "\n",
    "# Initialize empty torch tensor to hold all labels [text_id, language]\n",
    "labels = torch.zeros((len(text_ids) * len(languages),), dtype=torch.long).to('cuda')\n",
    "print(f'Labels tensor shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae48ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_per_lang.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9305ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load activations for all languages\n",
    "# id2langcode = {idx: lang for idx, lang in enumerate(languages)}\n",
    "# langcode2id = {lang: idx for idx, lang in enumerate(languages)}\n",
    "# text_idx = 0\n",
    "# for lang_idx, lang in tqdm(enumerate(languages), total=len(languages), desc='Loading activations for all languages'):\n",
    "# \tfor layer_id in range(-1, num_layers):\n",
    "# \t\tif layer_id == -1:\n",
    "# \t\t\tpaths = sorted(glob.glob(f'outputs/topic_classification/{model_name}/{extraction_mode}/{lang}/*/{token_position}/layer_embed_tokens.pt'))\n",
    "# \t\telse:\n",
    "# \t\t\tpaths = sorted(glob.glob(f'outputs/topic_classification/{model_name}/{extraction_mode}/{lang}/*/{token_position}/layer_{layer_id}.pt'))\n",
    "# \t\tif len(paths) != len(text_ids):\n",
    "# \t\t\tprint(f\"Warning: Expected {len(text_ids)} files for language '{lang}' at layer {layer_id}, but found {len(paths)} files.\")\n",
    "# \t\t\tbreak\n",
    "# \t\tfor path in paths:\n",
    "# \t\t\tactivation = torch.load(path)\n",
    "# \t\t\tactivation_per_lang[text_idx, layer_id + 1, lang_idx, :] = activation.to('cuda')\n",
    "# \t\t\ttext_idx += 1\n",
    "# \t\ttext_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52984788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load activations for all languages\n",
    "text_idx = 0\n",
    "id2langcode = {idx: lang for idx, lang in enumerate(languages)}\n",
    "langcode2id = {lang: idx for idx, lang in enumerate(languages)}\n",
    "\n",
    "for lang_idx, lang in tqdm(enumerate(languages), total=len(languages), desc='Loading activations for all languages'):\n",
    "\tfor layer_id in range(-1, num_layers):\n",
    "\t\tif layer_id == -1:\n",
    "\t\t\tpaths = sorted(glob.glob(f'outputs/topic_classification/{model_name}/{extraction_mode}/{lang}/*/{token_position}/layer_embed_tokens.pt'))\n",
    "\t\telse:\n",
    "\t\t\tpaths = sorted(glob.glob(f'outputs/topic_classification/{model_name}/{extraction_mode}/{lang}/*/{token_position}/layer_{layer_id}.pt'))\n",
    "\t\tif len(paths) != len(text_ids):\n",
    "\t\t\tprint(f\"Warning: Expected {len(text_ids)} files for language '{lang}' at layer {layer_id}, but found {len(paths)} files.\")\n",
    "\t\t\tbreak\n",
    "\t\tfor text_idx_inner, path in enumerate(paths):\n",
    "\t\t\tactivation = torch.load(path)\n",
    "\t\t\t# Calculate the flat index for the reshaped tensor\n",
    "\t\t\tflat_idx = text_idx_inner * len(languages) + lang_idx\n",
    "\t\t\tactivation_per_lang[flat_idx, layer_id + 1, :] = activation.to('cuda')\n",
    "\t\t\t# Populate labels with language_id (only need to do this once per text-language pair)\n",
    "\t\t\tif layer_id == -1:\n",
    "\t\t\t\tlabels[flat_idx] = lang_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_per_lang.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ec1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_per_lang[:, layer_id + 1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7051f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.cluster import KMeans\n",
    "from cuml.metrics.cluster.silhouette_score import cython_silhouette_score\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all index of labels that have value 1\n",
    "indexes_lang1 = ((labels == 1) | (labels == 2)).nonzero(as_tuple=True)[0]\n",
    "# Take activations for those indexes\n",
    "activations_lang1 = activation_per_lang[indexes_lang1, layer_id + 1, :]\n",
    "labels_lang1 = labels[indexes_lang1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c402c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4. Calculate the silhouette score on the GPU\n",
    "# The function takes the data and the predicted labels as input.\n",
    "silhouette_score_matrix = torch.zeros((num_layers + 1, len(languages), len(languages))).to('cuda')\n",
    "for layer_id in tqdm(range(-1, num_layers)):\n",
    "\t# Calculate pairwise silhouette scores per language\n",
    "\tfor lang_idx1 in range(len(languages)):\n",
    "\t\tfor lang_idx2 in range(len(languages)):\n",
    "\t\t\tif lang_idx1 != lang_idx2:\n",
    "\t\t\t\t# Take all index of labels that have value 1\n",
    "\t\t\t\tindexes_lang1 = ((labels == lang_idx1) | (labels == lang_idx2)).nonzero(as_tuple=True)[0]\n",
    "\t\t\t\t# Take activations for those indexes\n",
    "\t\t\t\tactivations_lang1 = activation_per_lang[indexes_lang1, layer_id + 1, :]\n",
    "\t\t\t\tlabels_lang1 = labels[indexes_lang1]\n",
    "\t\t\t\t# Combine activations and labels to both one tensor\n",
    "\t\t\t\tscore = cython_silhouette_score(activations_lang1, labels_lang1)\n",
    "\t\t\t\t# Store score in matrix\n",
    "\t\t\t\tsilhouette_score_matrix[layer_id + 1, lang_idx1, lang_idx2] = score\n",
    "\n",
    "\t\t\t\t# print(f\"Silhouette Score between {id2langcode[lang_idx1]} and {id2langcode[lang_idx2]}: {score:.4f}\")\n",
    "\n",
    "# For this well-separated data, you should see a high score (e.g., > 0.8)\n",
    "# A high score indicates that the clusters are dense and well-separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e559eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score_matrix[1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e523e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store silhouette score matrix\n",
    "os.makedirs('outputs/silhouette_scores', exist_ok=True)\n",
    "torch.save(silhouette_score_matrix, f'outputs/silhouette_scores/{model_name}_{extraction_mode}_{token_position}_silhouette_scores.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2107220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top k minimum distance values in each layer with their language pair identifiers\n",
    "\n",
    "# Set k as a variable\n",
    "k = 50  # You can change this value as needed\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_min_distances = []\n",
    "all_min_distance_pairs = []\n",
    "\n",
    "# For each layer, find the top k minimum distances and corresponding language pairs\n",
    "for layer_id in range(-1, num_layers):\n",
    "\tlayer_distances = silhouette_score_matrix[layer_id + 1]\n",
    "\t\n",
    "\t# Create a mask to exclude diagonal elements and upper triangle (to avoid duplicates)\n",
    "\tmask = torch.tril(torch.ones_like(layer_distances, dtype=torch.bool), diagonal=-1)\n",
    "\t\n",
    "\t# Get all distances excluding diagonal and upper triangle, then flatten\n",
    "\tlower_triangle_distances = layer_distances[mask]\n",
    "\t\n",
    "\t# Get top k minimum distances\n",
    "\ttopk_min_values, topk_indices = torch.topk(lower_triangle_distances, k=k, largest=False)\n",
    "\t\n",
    "\t# Convert flat indices back to 2D coordinates\n",
    "\tlayer_min_distances = []\n",
    "\tlayer_min_pairs = []\n",
    "\t\n",
    "\t# Get indices of lower triangle elements\n",
    "\tlower_i, lower_j = torch.where(mask)\n",
    "\t\n",
    "\tfor idx in range(k):\n",
    "\t\tflat_idx = topk_indices[idx].item()\n",
    "\t\tlang_i_idx = lower_i[flat_idx].item()\n",
    "\t\tlang_j_idx = lower_j[flat_idx].item()\n",
    "\t\t\n",
    "\t\tlayer_min_distances.append(topk_min_values[idx].item())\n",
    "\t\tlayer_min_pairs.append((languages[lang_i_idx], languages[lang_j_idx]))\n",
    "\t\n",
    "\tall_min_distances.append(layer_min_distances)\n",
    "\tall_min_distance_pairs.append(layer_min_pairs)\n",
    "\n",
    "# Create a detailed DataFrame\n",
    "detailed_results = []\n",
    "for layer_id in range(-1, num_layers):\n",
    "\tfor rank in range(k):\n",
    "\t\tlang1, lang2 = all_min_distance_pairs[layer_id + 1][rank]\n",
    "\t\tdetailed_results.append({\n",
    "\t\t\t'Layer': layer_id,\n",
    "\t\t\t'Rank': rank + 1,\n",
    "\t\t\t'Min_Distance': all_min_distances[layer_id + 1][rank],\n",
    "\t\t\t'Language_Pair': f\"{LANGCODE2LANGNAME[lang1]} ({lang1}) - {LANGCODE2LANGNAME[lang2]} ({lang2})\"\n",
    "\t\t})\n",
    "\n",
    "results_df = pd.DataFrame(detailed_results)\n",
    "\n",
    "print(f\"Top {k} minimum distances per layer:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Also print a summary view grouped by layer\n",
    "print(\"\\nSummary by layer:\")\n",
    "for layer_id in range(-1, num_layers):\n",
    "\tprint(f\"\\nLayer {layer_id:2d}:\")\n",
    "\tfor rank in range(k):\n",
    "\t\tlang1, lang2 = all_min_distance_pairs[layer_id + 1][rank]\n",
    "\t\tdistance = all_min_distances[layer_id + 1][rank]\n",
    "\t\tprint(f\"  {rank+1}. {distance:.6f} - {LANGCODE2LANGNAME[lang1]} ({lang1}) & {LANGCODE2LANGNAME[lang2]} ({lang2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and std deviation of the distances for each layer\n",
    "layer_means = silhouette_score_matrix.mean(dim=(1, 2)).cpu().numpy()\n",
    "layer_stds = silhouette_score_matrix.std(dim=(1, 2)).cpu().numpy()\n",
    "\n",
    "# Create a DataFrame to display the statistics\n",
    "layer_stats_df = pd.DataFrame({\n",
    "\t'Layer': range(-1, num_layers),\n",
    "\t'Mean_Distance': layer_means,\n",
    "\t'Std_Distance': layer_stds\n",
    "})\n",
    "\n",
    "print(\"Layer-wise Distance Statistics:\")\n",
    "print(layer_stats_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_per_family = {}\n",
    "for family, langs in EXP3_CONFIG['languages'].items():\n",
    "    lengths_per_family[family] = len(langs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d335e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_per_family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7644a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap per layer of the silhouette scores between languages, make a subplot for each layer with 4 columns\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Find global min and max across all layers for consistent scale\n",
    "global_min = float('inf')\n",
    "global_max = float('-inf')\n",
    "for layer_id in range(-1, num_layers):\n",
    "\tlayer_distances = silhouette_score_matrix[layer_id + 1]\n",
    "\tglobal_min = min(global_min, layer_distances.min().item())\n",
    "\tglobal_max = max(global_max, layer_distances.max().item())\n",
    "\n",
    "fig, axes = plt.subplots(nrows=(num_layers + 1) // 4 + 1, ncols=4, figsize=(20, 5 * ((num_layers + 1) // 4 + 1)))\n",
    "axes = axes.flatten()\n",
    "for layer_id in tqdm(range(0, num_layers)):\n",
    "\tlayer_distances = silhouette_score_matrix[layer_id + 1]\n",
    "\tsns.heatmap(layer_distances.cpu().numpy(), \n",
    "\t\txticklabels=[LANGCODE2LANGNAME[lang] for lang in languages], \n",
    "\t\tyticklabels=[LANGCODE2LANGNAME[lang] for lang in languages], \n",
    "\t\tax=axes[layer_id], \n",
    "\t\tcmap='viridis_r',\n",
    "\t\tvmin=global_min,\n",
    "\t\tvmax=global_max,\n",
    "\t\t# annot=True,\n",
    "\t\t# fmt='.2f',\n",
    "\t)\n",
    "\taxes[layer_id].set_title(f'Layer {layer_id} Silhouette Scores')\n",
    "\tplt.setp(axes[layer_id].get_xticklabels(), rotation=90, ha='right', rotation_mode='anchor')\n",
    "\n",
    "\t# Setup fontsize of the x and y label\n",
    "\taxes[layer_id].tick_params(axis='x', labelsize=5)\n",
    "\taxes[layer_id].tick_params(axis='y', labelsize=5)\n",
    "\t\n",
    "\t# Add bolded grid lines to separate language families\n",
    "\t# Calculate cumulative positions for family boundaries\n",
    "\tcumulative_langs = 0\n",
    "\tfor family, num_langs in lengths_per_family.items():\n",
    "\t\tcumulative_langs += num_langs\n",
    "\t\t# Draw horizontal and vertical lines at family boundaries\n",
    "\t\taxes[layer_id].axhline(cumulative_langs, color='black', linewidth=1.5)\n",
    "\t\taxes[layer_id].axvline(cumulative_langs, color='black', linewidth=1.5)\n",
    "\n",
    "\t# if layer_id + 1 == 3:\n",
    "\t# \tbreak\n",
    "\t\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'silhouette_scores_heatmap_{model_name}_testing.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20abb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Make 5 bins of the silhouette scores and color the heatmap accordingly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Find global min and max across all layers for consistent scale\n",
    "global_min = float('inf')\n",
    "global_max = float('-inf')\n",
    "for layer_id in range(-1, num_layers):\n",
    "\tlayer_distances = silhouette_score_matrix[layer_id + 1]\n",
    "\t# Exclude diagonal elements (which are 0)\n",
    "\tmask_no_diag = ~torch.eye(len(languages), dtype=torch.bool, device='cuda')\n",
    "\tnon_diag_values = layer_distances[mask_no_diag]\n",
    "\tglobal_min = min(global_min, non_diag_values.min().item())\n",
    "\tglobal_max = max(global_max, non_diag_values.max().item())\n",
    "\n",
    "# Create 3 bins\n",
    "n_bins = 5\n",
    "bin_edges = np.linspace(global_min, global_max, n_bins + 1)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=(num_layers + 1) // 4 + 1, ncols=4, figsize=(20, 5 * ((num_layers + 1) // 4 + 1)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for layer_id in tqdm(range(0, num_layers)):\n",
    "\tlayer_distances = silhouette_score_matrix[layer_id + 1].cpu().numpy()\n",
    "\t\n",
    "\t# Discretize the distances into bins\n",
    "\tbinned_distances = np.digitize(layer_distances, bin_edges) - 1\n",
    "\tbinned_distances = np.clip(binned_distances, 0, n_bins - 1)\n",
    "\t\n",
    "\tsns.heatmap(binned_distances, \n",
    "\t\txticklabels=[LANGCODE2LANGNAME[lang] for lang in languages], \n",
    "\t\tyticklabels=[LANGCODE2LANGNAME[lang] for lang in languages], \n",
    "\t\tax=axes[layer_id], \n",
    "\t\tcmap='viridis_r',\n",
    "\t\tvmin=0,\n",
    "\t\tvmax=n_bins - 1,\n",
    "\t\tcbar_kws={'label': 'Bin', 'ticks': range(n_bins)}\n",
    "\t)\n",
    "\taxes[layer_id].set_title(f'Layer {layer_id} Binned Silhouette Scores')\n",
    "\tplt.setp(axes[layer_id].get_xticklabels(), rotation=90, ha='right', rotation_mode='anchor')\n",
    "\n",
    "\t# Setup fontsize of the x and y label\n",
    "\taxes[layer_id].tick_params(axis='x', labelsize=5)\n",
    "\taxes[layer_id].tick_params(axis='y', labelsize=5)\n",
    "\t\n",
    "\t# Add bolded grid lines to separate language families\n",
    "\tcumulative_langs = 0\n",
    "\tfor family, num_langs in lengths_per_family.items():\n",
    "\t\tcumulative_langs += num_langs\n",
    "\t\taxes[layer_id].axhline(cumulative_langs, color='black', linewidth=1.5)\n",
    "\t\taxes[layer_id].axvline(cumulative_langs, color='black', linewidth=1.5)\n",
    "\t\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'silhouette_scores_binned{n_bins}_heatmap_{model_name}.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Bin edges: {bin_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f11a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Create subplots for dendrograms across all layers\n",
    "fig, axes = plt.subplots(nrows=9, ncols=5, figsize=(35, 90))\n",
    "fig.suptitle('Hierarchical Clustering Dendrograms Across All Layers', fontsize=16)\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# First pass: find the maximum distance across all layers\n",
    "max_distance = 0\n",
    "linkage_results = []\n",
    "for layer_id in range(-1, num_layers):\n",
    "\tlinked = linkage(silhouette_score_matrix[layer_id + 1].cpu().numpy(), 'complete')\n",
    "\tlinkage_results.append(linked)\n",
    "\tmax_distance = max(max_distance, linked[:, 2].max())\n",
    "\n",
    "# Second pass: create dendrograms with consistent scale\n",
    "for layer_id in tqdm(range(0, num_layers)):\n",
    "\tax_idx = layer_id + 1\n",
    "\t\n",
    "\t# Create dendrogram in the corresponding subplot\n",
    "\tdendrogram(linkage_results[ax_idx], \n",
    "\t\t\t   labels=[LANGCODE2LANGNAME[lang] for lang in languages], \n",
    "\t\t\t   orientation='right',\n",
    "\t\t\t   ax=axes[ax_idx - 1])\n",
    "\t\n",
    "\taxes[ax_idx - 1].set_title(f\"Layer {layer_id}\", fontsize=12)\n",
    "\taxes[ax_idx - 1].tick_params(axis='y', labelsize=6)\n",
    "\taxes[ax_idx - 1].tick_params(axis='x', labelsize=6)\n",
    "\taxes[ax_idx - 1].set_xlim([0, max_distance])  # Set consistent x-axis limits\n",
    "\n",
    "# Save image\n",
    "image_path = f'plot/silhouette_dendrograms_{model_name}_70lang.png'\n",
    "os.makedirs('plot', exist_ok=True)\n",
    "plt.savefig(image_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21974268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# Create subplots for dendrograms across all layers\n",
    "fig, axes = plt.subplots(nrows=9, ncols=5, figsize=(25, 80))\n",
    "fig.suptitle('Hierarchical Clustering Dendrograms Across All Layers', fontsize=16)\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "for layer_id in range(-1, num_layers):\n",
    "\tax_idx = layer_id + 1\n",
    "\t\n",
    "\t# Perform clustering for this layer\n",
    "\tlinked = linkage(silhouette_score_matrix[layer_id + 1].cpu().numpy(), 'complete')\n",
    "\t\n",
    "\t# Create dendrogram in the corresponding subplot\n",
    "\tdendrogram(linked, \n",
    "\t\t\t   labels=[LANGCODE2LANGNAME[lang] for lang in languages], \n",
    "\t\t\t   orientation='right',\n",
    "\t\t\t   ax=axes[ax_idx])\n",
    "\t\n",
    "\taxes[ax_idx].set_title(f\"Layer {layer_id}\", fontsize=12)\n",
    "\taxes[ax_idx].tick_params(axis='y', labelsize=8)\n",
    "\taxes[ax_idx].tick_params(axis='x', labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activation-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
