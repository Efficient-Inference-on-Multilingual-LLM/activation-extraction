{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80bd5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097ed320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c783b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseActivationSaver:\n",
    "\t\"\"\"\n",
    "\tBase class for saving activations from different models.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, base_save_dir: str, task_id: str, data_split: str, model_name: str, prompt_id: str):\n",
    "\t\tself.task_id = task_id\n",
    "\t\tself.data_split = data_split\n",
    "\t\tself.model_name = model_name\n",
    "\t\tself.prompt_id = prompt_id\n",
    "\t\tself.base_save_dir = base_save_dir\n",
    "\t\tself.current_id = None\n",
    "\t\tself.current_lang = None\n",
    "\n",
    "\tdef set_id(self, new_id):\n",
    "\t\tself.current_id = new_id\n",
    "\n",
    "\tdef set_lang(self, new_lang):\n",
    "\t\tself.current_lang = new_lang\n",
    "\n",
    "\tdef hook_fn(self, module, input, output, layer_id):\n",
    "\t\traise NotImplementedError(\"This method should be overridden by subclasses.\")\n",
    "\n",
    "\tdef pre_hook_fn(self, module, input, layer_id):\n",
    "\t\traise NotImplementedError(\"This method should be overridden by subclasses.\")\n",
    "\n",
    "\t# Check if activations for an instance already exist\n",
    "\tdef check_exists(self):\n",
    "\t\tpath_last_token = os.path.join(self.base_save_dir, self.task_id, self.data_split, self.model_name.split('/')[-1], self.prompt_id, self.current_lang, self.current_id, \"last_token\")\n",
    "\t\tpath_average = os.path.join(self.base_save_dir, self.task_id, self.data_split, self.model_name.split('/')[-1], self.prompt_id, self.current_lang, self.current_id, \"average\")\n",
    "\t\tcheck_files = os.listdir(path_last_token) if os.path.exists(path_last_token) else []\n",
    "\n",
    "\t\t# # Check if each extraction exist\n",
    "\t\t# post_attn_files = [f for f in check_files if 'postattn' in f]\n",
    "\t\t# post_mlp_files = [f for f in check_files if 'postmlp' in f]\n",
    "\t\t# embed_token_file = [f for f in check_files if 'embed_tokens' in f]\n",
    "\t\t# if len(post_attn_files) == 0 or len(post_mlp_files) == 0 or len(embed_token_file) == 0:\n",
    "\t\t# \treturn False\n",
    "\t\t\n",
    "\t\t# Check average directory files\n",
    "\t\tcheck_files_avg = os.listdir(path_average) if os.path.exists(path_average) else []\n",
    "\n",
    "\t\t# If there are any files, return True\n",
    "\t\treturn bool(check_files) and bool(check_files_avg)\n",
    "\n",
    "\tdef _save_activation_last_token(self, tensor, layer_id):\n",
    "\t\tpath = os.path.join(self.base_save_dir, self.task_id, self.data_split, self.model_name.split('/')[-1], self.prompt_id, self.current_lang, self.current_id, \"last_token\")\n",
    "\t\tos.makedirs(path, exist_ok=True)\n",
    "\t\tsave_path = os.path.join(path, f\"layer_{layer_id}.pt\")\n",
    "\t\ttorch.save(tensor[0, -1, :].detach().cpu(), save_path)\n",
    "\n",
    "\tdef _save_activation_average(self, tensor, layer_id):\n",
    "\t\tpath = os.path.join(self.base_save_dir, self.task_id, self.data_split, self.model_name.split('/')[-1], self.prompt_id, self.current_lang, self.current_id, \"average\")\n",
    "\t\tos.makedirs(path, exist_ok=True)\n",
    "\t\tsave_path = os.path.join(path, f\"layer_{layer_id}.pt\")\n",
    "\t\ttorch.save(tensor[0].mean(dim=0).detach().cpu(), save_path)\n",
    "\t\n",
    "\tdef _check_set_id_lang(self, layer_id):\n",
    "\t\tif self.current_id is None:\n",
    "\t\t\tprint(f\"Warning: ID not set for layer {layer_id}\")\n",
    "\t\t\treturn False\n",
    "\n",
    "\t\tif self.current_lang is None:\n",
    "\t\t\tprint(f\"Warning: Language not set for layer {layer_id}\")\n",
    "\t\t\treturn False\n",
    "\t\t\n",
    "\t\treturn True\n",
    "\t\n",
    "class GeneralActivationSaver(BaseActivationSaver): # Handle Gemma3, Qwen, Pythia models (models that activation are returned in form of a tuple)\n",
    "\n",
    "\tdef hook_fn(self, module, input, output, layer_id):\n",
    "\t\tif self._check_set_id_lang(layer_id) is False:\n",
    "\t\t\treturn\n",
    "\t\ttry:\n",
    "\t\t\tself._save_activation_last_token(tensor=output[0] if isinstance(output, tuple) else output, layer_id=layer_id) # Unpack tensor from the tuple\n",
    "\t\t\tself._save_activation_average(tensor=output[0] if isinstance(output, tuple) else output, layer_id=layer_id)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error in hook_fn for layer {layer_id}: {e}\")\n",
    "\t\n",
    "\tdef pre_hook_fn(self, module, input, layer_id):\n",
    "\t\tif self._check_set_id_lang(layer_id) is False:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\t# Extract residual connection after attention (precisely after post-attention layer norm)\n",
    "\t\t\tself._save_activation_last_token(tensor=input[0] if isinstance(input, tuple) else input, layer_id=layer_id)\n",
    "\t\t\tself._save_activation_average(tensor=input[0] if isinstance(input, tuple) else input, layer_id=layer_id)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error in pre_hook_fn for layer {layer_id}: {e}\")\n",
    "\n",
    "class CohereDecoderActivationSaver(BaseActivationSaver):\n",
    "\tdef __init__(self, base_save_dir: str, task_id: str, data_split: str, model_name: str, prompt_id: str):\n",
    "\t\tsuper().__init__(base_save_dir, task_id, data_split, model_name, prompt_id)\n",
    "\t\tself.initial_residual = None\n",
    "\t\tself.attn_output = None\n",
    "\t\n",
    "\tdef hook_fn_embed_tokens(self, module, input, output, layer_id):\n",
    "\t\tif self._check_set_id_lang(layer_id) is False:\n",
    "\t\t\treturn\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\tself._save_activation_last_token(tensor=output[0] if isinstance(output, tuple) else output, layer_id=layer_id)\n",
    "\t\t\tself._save_activation_average(tensor=output[0] if isinstance(output, tuple) else output, layer_id=layer_id)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error in hook_fn_embed_tokens for layer {layer_id}: {e}\")\n",
    "\n",
    "\tdef hook_fn_set_initial_residual(self, module, input, output, layer_id):\n",
    "\t\tif self._check_set_id_lang(layer_id) is False:\n",
    "\t\t\treturn\n",
    "\t\t\n",
    "\t\tself.initial_residual = input[0] if isinstance(input, tuple) else input\n",
    "\t\n",
    "\tdef hook_fn_set_attn_output(self, module, input, output, layer_id):\n",
    "\t\tif self._check_set_id_lang(layer_id) is False:\n",
    "\t\t\treturn\n",
    "\t\tself.attn_output = output[0] if isinstance(output, tuple) else output\n",
    "\t\n",
    "\tdef hook_fn_final_output(self, module, input, output, layer_id):\n",
    "\t\tif self._check_set_id_lang(layer_id) is False:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\t# Compute residual post MLP\n",
    "\t\tif self.initial_residual is None or self.attn_output is None:\n",
    "\t\t\tprint(f\"Warning: Missing stored tensors for layer {layer_id}\")\n",
    "\t\t\traise ValueError(\"Stored tensors are None\")\n",
    "\t\t\n",
    "\t\toutput = output[0] if isinstance(output, tuple) else output\n",
    "\t\tresidual_post_mlp = self.initial_residual + self.attn_output + output\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tself._save_activation_last_token(tensor=self.attn_output, layer_id=layer_id.replace('residual-postmlp', 'residual-postattn'))\n",
    "\t\t\tself._save_activation_average(tensor=self.attn_output, layer_id=layer_id.replace('residual-postmlp', 'residual-postattn'))\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error in hook_fn_final_output for layer {layer_id.replace('residual-postmlp', 'residual-postattn')}: {e}\")\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\tself._save_activation_last_token(tensor=residual_post_mlp, layer_id=layer_id)\n",
    "\t\t\tself._save_activation_average(tensor=residual_post_mlp, layer_id=layer_id)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error in hook_fn_final_output for layer {layer_id}: {e}\")\n",
    "\n",
    "\t\t# Reset stored tensors\n",
    "\t\tself.initial_residual = None\n",
    "\t\tself.attn_output = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2773cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseHookedModel:\n",
    "\t\"\"\"\n",
    "\tBase class for hooking into different models.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, model_name: str, saver: BaseActivationSaver):\n",
    "\t\tdevice = \"cpu\"\n",
    "\t\tmodel_dtype = torch.float16\n",
    "\t\tif torch.cuda.is_available():\n",
    "\t\t\tdevice = \"cuda\"\n",
    "\t\t\tcompute_capability = torch.cuda.get_device_capability()[0]\n",
    "\n",
    "\t\t\t# Use bfloat16 if supported\n",
    "\t\t\tif compute_capability >= 8:\n",
    "\t\t\t\tmodel_dtype = torch.bfloat16\n",
    "\t\t\n",
    "\t\tself.model_name = model_name\n",
    "\t\tself.saver = saver\n",
    "\t\tself.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, device_map=device, cache_dir=os.getenv(\"HF_CACHE_DIR\"))\n",
    "\t\tself.model.eval()\n",
    "\t\n",
    "\tdef _setup_hooks(self):\n",
    "\t\traise NotImplementedError(\"This method should be overridden by subclasses.\")\n",
    "\t\n",
    "\tdef set_saver_id(self, new_id: int):\n",
    "\t\tself.saver.set_id(new_id)\n",
    "\n",
    "\tdef set_saver_lang(self, new_lang: str):\n",
    "\t\tself.saver.set_lang(new_lang)\n",
    "\t\t\n",
    "\tdef generate(self, inputs):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = self.model.generate(\n",
    "\t\t\t\t**inputs,\n",
    "\t\t\t\tmax_new_tokens=1,\n",
    "\t\t\t)\n",
    "\t\treturn outputs\n",
    "\n",
    "\t# Clear hooks for debugging purposes\n",
    "\tdef clear_hooks(self):\n",
    "\t\tif 'bloom' in self.model_name:\n",
    "\t\t\tfor i, layer in enumerate(self.model.transformer.h):\n",
    "\t\t\t\tlayer._forward_hooks.clear()\n",
    "\t\telse:\n",
    "\t\t\tself.model.model.embed_tokens._forward_hooks.clear()\n",
    "\t\t\tself.model.model.norm._forward_hooks.clear()\n",
    "\t\t\tfor i, layer in enumerate(self.model.model.layers):\n",
    "\t\t\t\tlayer._forward_hooks.clear()\n",
    "\t\t\t\tlayer._forward_pre_hooks.clear()\n",
    "\t\n",
    "class Gemma3MultimodalHookedModel(BaseHookedModel): # For gemma-3 >=4b\n",
    "\tdef __init__(self, model_name: str, saver: BaseActivationSaver):\n",
    "\t\tsuper().__init__(model_name, saver)\n",
    "\t\tself._setup_hooks()\n",
    "\n",
    "\tdef _setup_hooks(self):\n",
    "\t\tself.model.model.language_model.embed_tokens.register_forward_hook(lambda module, input, output, layer_id=\"embed_tokens\": self.saver.hook_fn(module, input, output, layer_id))\n",
    "\n",
    "\t\t# Decoder layers\n",
    "\t\tfor i, layer in enumerate(self.model.model.language_model.layers):\n",
    "\n",
    "\t\t\t# Final output of decoder layer hook\n",
    "\t\t\tlayer.register_forward_hook(lambda module, input, output, layer_id=f'residual-postmlp_{i}': self.saver.hook_fn(module, input, output, layer_id))\n",
    "\n",
    "\t\t\t# Post-attention layer norm pre-hook (residual post attention)\n",
    "\t\t\tlayer.pre_feedforward_layernorm.register_forward_pre_hook(lambda module, input, layer_id=f\"residual-postattn_{i}\": self.saver.pre_hook_fn(module, input, layer_id))\n",
    "\n",
    "class PythiaHookedModel(BaseHookedModel): # For pythia models\n",
    "\tdef __init__(self, model_name: str, saver: BaseActivationSaver):\n",
    "\t\tsuper().__init__(model_name, saver)\n",
    "\t\tself._setup_hooks()\n",
    "\n",
    "\tdef _setup_hooks(self):\n",
    "\t\t# Embedding layer\n",
    "\t\tself.model.gpt_neox.embed_in.register_forward_hook(lambda module, input, output, layer_id=\"embed_tokens\": self.saver.hook_fn(module, input, output, layer_id))\n",
    "\n",
    "\t\t# Decoder layers\n",
    "\t\tfor i, layer in enumerate(self.model.gpt_neox.layers):\n",
    "\n",
    "\t\t\t# Final output of decoder layer hook\n",
    "\t\t\tlayer.register_forward_hook(lambda module, input, output, layer_id=f'residual-postmlp_{i}': self.saver.hook_fn(module, input, output, layer_id))\n",
    "\n",
    "\t\t\t# Post-attention layer norm pre-hook (residual post attention)\n",
    "\t\t\tlayer.post_attention_layernorm.register_forward_pre_hook(lambda module, input, layer_id=f\"residual-postattn_{i}\": self.saver.pre_hook_fn(module, input, layer_id))\n",
    "\n",
    "class CohereDecoderHookedModel(BaseHookedModel): # For cohere decoder models\n",
    "\tdef __init__(self, model_name: str, saver: CohereDecoderActivationSaver):\n",
    "\t\tsuper().__init__(model_name, saver)\n",
    "\t\tself._setup_hooks()\n",
    "\n",
    "\tdef _setup_hooks(self):\n",
    "\t\t# Embedding layer\n",
    "\t\tself.model.model.embed_tokens.register_forward_hook(lambda module, input, output, layer_id=\"embed_tokens\": self.saver.hook_fn_embed_tokens(module, input, output, layer_id))\n",
    "\n",
    "\t\t# Decoder layers\n",
    "\t\tfor i, layer in enumerate(self.model.model.layers):\n",
    "\n",
    "\t\t\t# Pre-attention layer norm hook (residual pre attention)\n",
    "\t\t\tlayer.input_layernorm.register_forward_hook(lambda module, input, output, layer_id=f\"residual-preattn_{i}\": self.saver.hook_fn_set_initial_residual(module, input, output, layer_id))\n",
    "\n",
    "\t\t\t# Post-attention layer norm pre-hook (residual post attention)\n",
    "\t\t\tlayer.self_attn.register_forward_hook(lambda module, input, output, layer_id=f\"residual-postattn_{i}\": self.saver.hook_fn_set_attn_output(module, input, output, layer_id))\n",
    "\t\t\t\n",
    "\t\t\t# Final output of decoder layer hook\n",
    "\t\t\tlayer.register_forward_hook(lambda module, input, output, layer_id=f'residual-postmlp_{i}': self.saver.hook_fn_final_output(module, input, output, layer_id))\n",
    "\n",
    "class Qwen3HookedModel(BaseHookedModel): # For Qwen models\n",
    "\tdef __init__(self, model_name: str, saver: BaseActivationSaver):\n",
    "\t\tsuper().__init__(model_name, saver)\n",
    "\t\tself._setup_hooks()\n",
    "\n",
    "\tdef _setup_hooks(self):\n",
    "\t\t# Embedding layer\n",
    "\t\tself.model.model.embed_tokens.register_forward_hook(lambda module, input, output, layer_id=\"embed_tokens\": self.saver.hook_fn(module, input, output, layer_id))\n",
    "\n",
    "\t\t# Decoder layers\n",
    "\t\tfor i, layer in enumerate(self.model.model.layers):\n",
    "\n",
    "\t\t\t# Post-attention layer norm pre-hook (residual post attention)\n",
    "\t\t\tlayer.post_attention_layernorm.register_forward_pre_hook(lambda module, input, layer_id=f\"residual-postattn_{i}\": self.saver.pre_hook_fn(module, input, layer_id))\n",
    "\t\t\t\n",
    "\t\t\t# Final output of decoder layer hook\n",
    "\t\t\tlayer.register_forward_hook(lambda module, input, output, layer_id=f'residual-postmlp_{i}': self.saver.hook_fn(module, input, output, layer_id))\n",
    "\n",
    "class LlamaHookedModel(BaseHookedModel): # For Llama 3 models\n",
    "\tdef __init__(self, model_name: str, saver: BaseActivationSaver):\n",
    "\t\tsuper().__init__(model_name, saver)\n",
    "\t\tself._setup_hooks()\n",
    "\n",
    "\tdef _setup_hooks(self):\n",
    "\t\t# Embedding layer\n",
    "\t\tself.model.model.embed_tokens.register_forward_hook(lambda module, input, output, layer_id=\"embed_tokens\": self.saver.hook_fn(module, input, output, layer_id))\n",
    "\n",
    "\t\t# Decoder layers\n",
    "\t\tfor i, layer in enumerate(self.model.model.layers):\n",
    "\n",
    "\t\t\t# Post-attention layer norm pre-hook (residual post attention)\n",
    "\t\t\tlayer.post_attention_layernorm.register_forward_pre_hook(lambda module, input, layer_id=f\"residual-postattn_{i}\": self.saver.pre_hook_fn(module, input, layer_id))\n",
    "\t\t\t\n",
    "\t\t\t# Final output of decoder layer hook\n",
    "\t\t\tlayer.register_forward_hook(lambda module, input, output, layer_id=f'residual-postmlp_{i}': self.saver.hook_fn(module, input, output, layer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d502bf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# login huggingface\n",
    "from huggingface_hub import login\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6a499e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"cuda\", cache_dir=os.getenv(\"HF_CACHE_DIR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a2b9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lang = 'no_prompt'\n",
    "is_base_model = True\n",
    "data_split = 'dev'\n",
    "sample_size = 5\n",
    "output_dir = 'output_temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecfab7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model: meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7746fbf9adc547068b7358abade09908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde0b109b6bc427bae7d2e484312f220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609330c0fef94e838662e1e3ce064cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f2351232664fb88be7783b1babe1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a0cc30a0a243168687c157ac78cc35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f28774a5fe46d0bdef065d53996386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing activation for next token prediction task (ind_Latn):   0%|          | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing activation for next token prediction task (ind_Latn):  20%|██        | 1/5 [00:00<00:02,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing activation for next token prediction task (ind_Latn):  40%|████      | 2/5 [00:00<00:01,  2.97it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing activation for next token prediction task (ind_Latn):  60%|██████    | 3/5 [00:00<00:00,  4.36it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing activation for next token prediction task (ind_Latn):  80%|████████  | 4/5 [00:00<00:00,  5.58it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing activation for next token prediction task (ind_Latn): 100%|██████████| 5/5 [00:01<00:00,  4.61it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556f20490e534231b61a21305550a27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0887fb940a5f41239bdd32de23b8b629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing activation for next token prediction task (eng_Latn):   0%|          | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing activation for next token prediction task (eng_Latn):  20%|██        | 1/5 [00:00<00:00,  9.64it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing activation for next token prediction task (eng_Latn):  40%|████      | 2/5 [00:00<00:00,  9.55it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing activation for next token prediction task (eng_Latn):  60%|██████    | 3/5 [00:00<00:00,  9.53it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing activation for next token prediction task (eng_Latn):  80%|████████  | 4/5 [00:00<00:00,  9.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing activation for next token prediction task (eng_Latn): 100%|██████████| 5/5 [00:00<00:00,  9.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "print(f'Load model: {model_name}')\n",
    "\n",
    "if prompt_lang == 'all':\n",
    "\tprompt_id_saver = 'prompted'\n",
    "elif prompt_lang == 'no_prompt':\n",
    "\tprompt_id_saver = 'raw'\n",
    "else:\n",
    "\tprompt_id_saver = f'prompt_{prompt_lang}' \n",
    "\n",
    "if 'cohere' in model_name.lower():\n",
    "\tsaver = CohereDecoderActivationSaver(output_dir, task_id='next_token', data_split=data_split, model_name=model_name, prompt_id=prompt_id_saver)\n",
    "else:\n",
    "\tsaver = GeneralActivationSaver(output_dir, task_id='next_token', data_split=data_split, model_name=model_name, prompt_id=prompt_id_saver)\n",
    "\n",
    "if 'gemma-3' in model_name.lower():\n",
    "\thooked_model = Gemma3MultimodalHookedModel(model_name, saver=saver)\n",
    "elif 'cohere' in model_name.lower():\n",
    "\thooked_model = CohereDecoderHookedModel(model_name, saver=saver)\n",
    "elif 'pythia' in model_name.lower():\n",
    "\thooked_model = PythiaHookedModel(model_name, saver=saver)\n",
    "elif 'qwen' in model_name.lower():\n",
    "\thooked_model = Qwen3HookedModel(model_name, saver=saver)\n",
    "elif 'meta-llama' in model_name.lower():\n",
    "\thooked_model = LlamaHookedModel(model_name, saver=saver)\n",
    "else:\n",
    "\traise ValueError(f\"Model {model_name} not supported in this script.\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "languages = ['ind_Latn', 'eng_Latn']\n",
    "\n",
    "# Feed Forward\n",
    "# for lang in languages:\n",
    "for lang in languages:\n",
    "\t\n",
    "\t# Load Dataset\n",
    "\tdatasets_per_lang = {}\n",
    "\tif data_split == 'all':\n",
    "\t\tdatasets_per_lang_temp = {}\n",
    "\t\tdatasets_per_lang_temp[lang] = load_dataset(\"openlanguagedata/flores_plus\", lang, cache_dir=os.getenv(\"HF_CACHE_DIR\"))\n",
    "\t\tdatasets_per_lang[lang] = concatenate_datasets([datasets_per_lang_temp[lang]['dev'], datasets_per_lang_temp[lang]['devtest']])\n",
    "\telse:\n",
    "\t\tdatasets_per_lang[lang] = load_dataset(\"openlanguagedata/flores_plus\", lang, split=data_split, cache_dir=os.getenv(\"HF_CACHE_DIR\"))\n",
    "\t\n",
    "\t# Sample Dataset\n",
    "\tif sample_size:\n",
    "\t\tdatasets_per_lang[lang] = datasets_per_lang[lang].shuffle(seed=42).select(range(sample_size))\n",
    "\n",
    "\t# Load Prompt Template\n",
    "\tif prompt_lang == \"all\": \n",
    "\t\twith open(f'./prompts/next_token/{lang}.txt') as f:\n",
    "\t\t\tprompt_template = f.read()\n",
    "\telse:\n",
    "\t\twith open(f'./prompts/next_token/{prompt_lang}.txt') as f:\n",
    "\t\t\tprompt_template = f.read()\n",
    "\t\n",
    "\t# Iterate Through Each Instance\n",
    "\tfor instance in tqdm(datasets_per_lang[lang], desc=f\"Processing activation for next token prediction task ({lang})\"):\n",
    "\t\t# Set ID and Language in Saver\n",
    "\t\thooked_model.set_saver_id(str(instance['id']))\n",
    "\t\thooked_model.set_saver_lang(lang)\n",
    "\n",
    "\t\t# Check if activations already exist\n",
    "\t\tif saver.check_exists():\n",
    "\t\t\tprint(f\"Activations already exist for ID {instance['id']} in language {lang}. Skipping...\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# Build Prompt Based on Template\n",
    "\t\tprompt = prompt_template.replace(\"{text}\", instance['text'])\n",
    "\n",
    "\t\t# Inference\n",
    "\t\tif is_base_model or 'bloom' in model_name:\n",
    "\t\t\ttext = prompt\n",
    "\t\telse:\n",
    "\t\t\t\n",
    "\t\t\t# Gemma2 does not support system message\n",
    "\t\t\tif 'google/gemma-2' in model_name.lower():\n",
    "\t\t\t\tmessages = [\n",
    "\t\t\t\t\t{'role': 'user', 'content': prompt}\n",
    "\t\t\t\t]\n",
    "\t\t\telse:\n",
    "\t\t\t\tmessages = [\n",
    "\t\t\t\t\t{'role': 'system', 'content': ''},\n",
    "\t\t\t\t\t{'role': 'user', 'content': prompt}\n",
    "\t\t\t\t]\n",
    "\n",
    "\t\t\tif 'meta-llama' in model_name.lower():\n",
    "\t\t\t\tuser_prompt = messages[-1]['content']\n",
    "\t\t\t\ttext = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "\t\t\telse:\n",
    "\t\t\t\ttext = tokenizer.apply_chat_template(\n",
    "\t\t\t\t\tmessages,\n",
    "\t\t\t\t\ttokenize=False,\n",
    "\t\t\t\t\tadd_generation_prompt=True,\n",
    "\t\t\t\t\tenable_thinking=False \n",
    "\t\t\t\t)\n",
    "\t\t\n",
    "\t\tinputs = tokenizer([text], return_tensors=\"pt\").to(hooked_model.model.device)\n",
    "\t\t_ = hooked_model.generate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be4f56a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activation-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
